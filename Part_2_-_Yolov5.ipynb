{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF9hBFoUcZbz"
      },
      "source": [
        "# 1. Setup & Data Import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej_BeUt8cUkn"
      },
      "source": [
        "## 1.1. Data Import into the execution environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content'\n",
        "\n",
        "%mkdir RoadDamageDataset \n",
        "%cd RoadDamageDataset\n",
        "\n",
        "# train set\n",
        "!wget -c https://mycityreport.s3-ap-northeast-1.amazonaws.com/02_RoadDamageDataset/public_data/IEEE_bigdata_RDD2020/train.tar.gz\n",
        "!tar xf train.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jnEPG9gjWJz",
        "outputId": "1dede65b-18d2-42dd-c5a4-6c2f6c705186"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/RoadDamageDataset\n",
            "--2022-09-05 08:04:05--  https://mycityreport.s3-ap-northeast-1.amazonaws.com/02_RoadDamageDataset/public_data/IEEE_bigdata_RDD2020/train.tar.gz\n",
            "Resolving mycityreport.s3-ap-northeast-1.amazonaws.com (mycityreport.s3-ap-northeast-1.amazonaws.com)... 3.5.154.11\n",
            "Connecting to mycityreport.s3-ap-northeast-1.amazonaws.com (mycityreport.s3-ap-northeast-1.amazonaws.com)|3.5.154.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1472626254 (1.4G) [application/x-tar]\n",
            "Saving to: â€˜train.tar.gzâ€™\n",
            "\n",
            "train.tar.gz        100%[===================>]   1.37G  12.4MB/s    in 1m 54s  \n",
            "\n",
            "2022-09-05 08:06:01 (12.3 MB/s) - â€˜train.tar.gzâ€™ saved [1472626254/1472626254]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Google Drive Connection and yolov5 Clone"
      ],
      "metadata": {
        "id": "2XqrFQGtmt1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/MyDrive'\n",
        "\n",
        "# Creation of the folder\n",
        "%mkdir RDD_Final \n",
        "%cd RDD_Final\n",
        "\n",
        "# Git Clone\n",
        "!git clone https://github.com/ultralytics/yolov5  # clone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEiuxL9EfcNW",
        "outputId": "d61f030f-80df-4cc8-fe29-263b791e6d45"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n",
            "mkdir: cannot create directory â€˜RDD_Finalâ€™: File exists\n",
            "/content/drive/MyDrive/RDD_Final\n",
            "fatal: destination path 'yolov5' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "104R40u6pmOC",
        "outputId": "49e1a9b8-5673-4097-9d22-2fee6f657589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/RDD_Final/yolov5\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.6 MB 15.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Installing and importing libraries\n",
        "%cd '/content/drive/MyDrive/RDD_Final/yolov5'\n",
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "from IPython.display import Image  # for displaying images\n",
        "import os \n",
        "import random\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xml.etree.ElementTree as ET\n",
        "from xml.dom import minidom\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from cmath import inf\n",
        "\n",
        "random.seed(108)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfilhcP-LhtW"
      },
      "source": [
        "# 2. Data management\n",
        "\n",
        "## 2.1. Moving files from country sub-folders to global folders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating 2 folders to put together all the data (annotations and images) from the 3 countries\n",
        "%cd /content/RoadDamageDataset/train/\n",
        "%mkdir annotations \n",
        "%mkdir images "
      ],
      "metadata": {
        "id": "yoO7xXo8R766",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e26dd8-bf4f-4021-b281-e30c6cf1a540"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RoadDamageDataset/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Moving all images and annotations from Country folders to the main folders\n",
        "Country = [\"Czech\", \"India\", \"Japan\"]\n",
        "\n",
        "for c in Country : \n",
        "\n",
        "  # Annotations \n",
        "\n",
        "  # Define the source and destination path\n",
        "  source = \"/content/RoadDamageDataset/train/\" + c + \"/annotations/xmls/\"\n",
        "  destination = \"/content/RoadDamageDataset/train/annotations/\"\n",
        "  \n",
        "  # Code to move the files from sub-folder to main folder.\n",
        "  files = os.listdir(source)\n",
        "  for file in files:\n",
        "    file_name = os.path.join(source, file)\n",
        "    shutil.move(file_name, destination)\n",
        "\n",
        "  # Images \n",
        "  \n",
        "  # Define the source and destination path\n",
        "  source = \"/content/RoadDamageDataset/train/\" + c + \"/images/\"\n",
        "  destination = \"/content/RoadDamageDataset/train/images/\"\n",
        "  \n",
        "  # Code to move the files from sub-folder to main folder.\n",
        "  files = os.listdir(source)\n",
        "  for file in files:\n",
        "    file_name = os.path.join(source, file)\n",
        "    shutil.move(file_name, destination)\n"
      ],
      "metadata": {
        "id": "hsfEzZMMYHVM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDyMYxzJc2DA"
      },
      "source": [
        "## 2.2. Annotation to YoloV5 format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l7DezgmQc1nv"
      },
      "outputs": [],
      "source": [
        "# Function to get the data from XML Annotation\n",
        "\n",
        "def extract_info_from_xml(xml_file):\n",
        "    root = ET.parse(xml_file).getroot()\n",
        "    \n",
        "    # Initialise the info dict \n",
        "    info_dict = {}\n",
        "    info_dict['bboxes'] = []\n",
        "\n",
        "    # Parse the XML Tree\n",
        "    for elem in root:\n",
        "        # Get the file name \n",
        "        if elem.tag == \"filename\":\n",
        "            info_dict['filename'] = elem.text\n",
        "            \n",
        "        # Get the image size\n",
        "        elif elem.tag == \"size\":\n",
        "            image_size = []\n",
        "            for subelem in elem:\n",
        "                image_size.append(int(subelem.text))\n",
        "            \n",
        "            info_dict['image_size'] = tuple(image_size)\n",
        "        \n",
        "        # Get details of the bounding box \n",
        "        elif elem.tag == \"object\":\n",
        "            bbox = {}\n",
        "            for subelem in elem:\n",
        "                if subelem.tag == \"name\":\n",
        "                    bbox[\"class\"] = subelem.text\n",
        "                    \n",
        "                elif subelem.tag == \"bndbox\":\n",
        "                    for subsubelem in subelem:\n",
        "                        bbox[subsubelem.tag] = int(subsubelem.text)            \n",
        "            info_dict['bboxes'].append(bbox)\n",
        "    \n",
        "    return info_dict\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_UBhqtjCLoWp"
      },
      "outputs": [],
      "source": [
        "# Dictionary that maps class names to IDs\n",
        "\n",
        "# Each DXX class corresponds to one road default ;\n",
        "# DOO/D01 : longitudinal cracks\n",
        "# D10/D11 : lateral cracks\n",
        "# D20 : aligator cracks\n",
        "# D40 : pot hole\n",
        "# D43/D44 : white/yellow lines\n",
        "# D50 : manholes\n",
        "# D0w0 : others\n",
        "class_name_to_id_mapping = {\"D00\": 0,\n",
        "                            \"D01\": 0,\n",
        "                           \"D10\": 1,\n",
        "                            \"D11\": 1,\n",
        "                           \"D20\": 2,\n",
        "                           \"D40\": 3,\n",
        "                           \"D43\": 4,\n",
        "                            \"D44\": 4,\n",
        "                            \"D50\": 5,\n",
        "                            \"D0w0\": 6\n",
        "                            }\n",
        "\n",
        "# Convert the info dict to the required yolo format and write it to disk\n",
        "def convert_to_yolov5(info_dict, path):\n",
        "    print_buffer = []\n",
        "    \n",
        "    # For each bounding box\n",
        "    for b in info_dict[\"bboxes\"]:\n",
        "        try:\n",
        "            class_id = class_name_to_id_mapping[b[\"class\"]]\n",
        "        except KeyError:\n",
        "            print(\"Invalid Class. Must be one from \", class_name_to_id_mapping.keys())\n",
        "        \n",
        "        # Transform the bbox co-ordinates as per the format required by YOLO v5\n",
        "        b_center_x = (b[\"xmin\"] + b[\"xmax\"]) / 2 \n",
        "        b_center_y = (b[\"ymin\"] + b[\"ymax\"]) / 2\n",
        "        b_width    = (b[\"xmax\"] - b[\"xmin\"])\n",
        "        b_height   = (b[\"ymax\"] - b[\"ymin\"])\n",
        "        \n",
        "        # Normalise the co-ordinates by the dimensions of the image\n",
        "        # if len(info_dict['image_size']) == 2:\n",
        "        #     image_w, image_h = info_dict[\"image_size\"]\n",
        "        # elif 'India' in info_dict['filename']:\n",
        "        #     image_c, image_w, image_h = info_dict[\"image_size\"]\n",
        "        # else:\n",
        "        #     image_w, image_h, image_c  = info_dict[\"image_size\"]\n",
        "        image_w = info_dict[\"image_size\"][1]\n",
        "        image_h = info_dict[\"image_size\"][1]\n",
        "\n",
        "        b_center_x /= image_w \n",
        "        b_center_y /= image_h \n",
        "        b_width    /= image_w \n",
        "        b_height   /= image_h \n",
        "        \n",
        "        #Write the bbox details to the file \n",
        "        print_buffer.append(\"{} {:.3f} {:.3f} {:.3f} {:.3f}\".format(class_id, b_center_x, b_center_y, b_width, b_height))\n",
        "        \n",
        "    # Name of the file which we have to save \n",
        "    save_file_name = os.path.join(path, info_dict[\"filename\"].replace(\"jpg\", \"txt\"))\n",
        "    #save_file_name = os.path.join(info_dict[\"filename\"].replace(\"jpg\", \"txt\"))\n",
        "    \n",
        "    # Save the annotation to disk\n",
        "    print(\"\\n\".join(print_buffer), file= open(save_file_name, \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/RoadDamageDataset/train'\n",
        "\n",
        "base_path = '/content/RoadDamageDataset/train/'\n",
        "\n",
        "cls_names = []\n",
        "total_images = 0\n",
        "    \n",
        "file_list = [filename for filename in os.listdir(base_path + '/annotations/') if not filename.startswith('.')]\n",
        "\n",
        "for file in file_list:\n",
        "\n",
        "      total_images = total_images + 1\n",
        "      if file =='.DS_Store':\n",
        "          pass\n",
        "      else:\n",
        "          infile_xml = open(base_path + '/annotations/' +file)\n",
        "          tree = ET.parse(infile_xml)\n",
        "          root = tree.getroot()\n",
        "          for obj in root.iter('object'):\n",
        "              cls_name = obj.find('name').text\n",
        "              cls_names.append(cls_name)\n",
        "\n",
        "print(\"total\")\n",
        "print(\"# of imagesï¼š\" + str(total_images))\n",
        "print(\"# of labelsï¼š\" + str(len(cls_names)))"
      ],
      "metadata": {
        "id": "Q11W_AKeVeQ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0cc2415-2f81-4547-aef4-a407b123e7f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RoadDamageDataset/train\n",
            "total\n",
            "# of imagesï¼š21041\n",
            "# of labelsï¼š34702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/RoadDamageDataset/train/'\n",
        "\n",
        "# Get the annotations\n",
        "annotations = [os.path.join(base_path + '/annotations/', x) for x in os.listdir(base_path + '/annotations/') if x[-3:] == \"xml\"]\n",
        "annotations.sort()\n",
        "\n",
        "# Convert and save the annotations\n",
        "for ann in tqdm(annotations):\n",
        "  info_dict = extract_info_from_xml(ann)\n",
        "  convert_to_yolov5(info_dict, os.path.join(base_path + '/annotations/'))\n",
        "annotations = [os.path.join(base_path + '/annotations/', x) for x in os.listdir(base_path + '/annotations/') if x[-3:] == \"txt\"]"
      ],
      "metadata": {
        "id": "6fIqTyExU21m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fccea5a-604f-4746-ce97-3c438c6d6c50"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21041/21041 [00:03<00:00, 6737.54it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Dividing data into Train, Test, Validation sets"
      ],
      "metadata": {
        "id": "RWqQryjMo4SL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read images and annotations\n",
        "base_path = '/content/RoadDamageDataset/train/'\n",
        "\n",
        "images = []\n",
        "annotations = []\n",
        "\n",
        "images.append([os.path.join('images', x) for x in os.listdir(os.path.join(base_path + '/images'))])\n",
        "annotations.append([os.path.join('annotations', x) for x in os.listdir(os.path.join(base_path + '/annotations')) if x[-3:] == \"txt\"])"
      ],
      "metadata": {
        "id": "RtKmSp2MVXrv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jIRCynympmOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98fa108d-931e-4675-eaad-6c1523102ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21041\n"
          ]
        }
      ],
      "source": [
        "images = images[0]\n",
        "annotations = annotations[0]\n",
        "images.sort()\n",
        "annotations.sort()\n",
        "\n",
        "print(len(images))\n",
        "# Split the dataset into train-valid-test splits \n",
        "train_images, val_images, train_annotations, val_annotations = train_test_split(images, annotations, test_size = 0.2, random_state = 2)\n",
        "val_images, test_images, val_annotations, test_annotations = train_test_split(val_images, val_annotations, test_size = 0.5, random_state = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oM0rG-H8pmOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbc7171b-13c2-4daa-8eb5-48edc14f86ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RoadDamageDataset\n"
          ]
        }
      ],
      "source": [
        "%cd /content/RoadDamageDataset/\n",
        "\n",
        "!mkdir Train Val Test "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tf8TgmsXpmOK"
      },
      "outputs": [],
      "source": [
        "#Utility function to copy images in designated folders\n",
        "def move_files_to_folder(list_of_files, destination_folder):\n",
        "    for f in list_of_files:\n",
        "        try:\n",
        "            shutil.move(f, destination_folder)\n",
        "        except:\n",
        "            print(f)\n",
        "            assert False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "B_KEmV0tpmOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "089bd339-a167-4931-a6ee-7dca3f79b56d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RoadDamageDataset/train\n"
          ]
        }
      ],
      "source": [
        "%cd /content/RoadDamageDataset/train/\n",
        "\n",
        "# Move the images splits into their folders\n",
        "move_files_to_folder(train_images, '/content/RoadDamageDataset/Train')\n",
        "move_files_to_folder(val_images, '/content/RoadDamageDataset/Val')\n",
        "move_files_to_folder(test_images, '/content/RoadDamageDataset/Test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IBldo6IMpmOL"
      },
      "outputs": [],
      "source": [
        "# Move the annotations splits into their folders\n",
        "move_files_to_folder(train_annotations, '/content/RoadDamageDataset/Train')\n",
        "move_files_to_folder(val_annotations, '/content/RoadDamageDataset/Val')\n",
        "move_files_to_folder(test_annotations, '/content/RoadDamageDataset/Test')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. YoloV5\n",
        "\n",
        "Important : At that point, copy paste the file road_damage.yaml into the folder /content/drive/MyDrive/RDD_Final/yolov5/data"
      ],
      "metadata": {
        "id": "26bO-SF_pBvc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iqMQVEKEpmOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3a99d72-5e52-4ae8-ecdc-82dfe374a996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.2-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.8 MB 10.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 47.5 MB/s \n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181 kB 60.0 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 66.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 71.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 71.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 54.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 67.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 70.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156 kB 69.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=d8e2c56682256270ad49ca533c97a79ca971639dd3992a6b3b7a15f9f9fa42e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.2\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/wandb\", line 8, in <module>\n",
            "    sys.exit(cli())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1259, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/cli/cli.py\", line 97, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/cli/cli.py\", line 245, in login\n",
            "    wandb.login(relogin=relogin, key=key, anonymous=anon_mode, host=host, force=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_login.py\", line 76, in login\n",
            "    configured = _login(**kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_login.py\", line 291, in _login\n",
            "    wlogin.configure_api_key(key)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_login.py\", line 175, in configure_api_key\n",
            "    apikey.write_key(self._settings, key)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/apikey.py\", line 224, in write_key\n",
            "    raise ValueError(\"API key must be 40 characters long, yours was %s\" % len(key))\n",
            "ValueError: API key must be 40 characters long, yours was 18\n"
          ]
        }
      ],
      "source": [
        "# Facultatif : Connection to WandB\n",
        "%pip install wandb\n",
        "!wandb login --relogin votre_code_d_acces"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/RDD_Final/yolov5/\n",
        "# Parameters to update: \n",
        "  # Epochs \n",
        "  # Hyp : hyperparameters to consider - 3 are available hyp.scratch-high.yaml, hyp.scratch-med.yaml, hyp.scratch-low.yaml but you can also consider to create your own .yaml with your own hyperparameters\n",
        "  # Weights : weights to consider - 5 are available yolov5s.pt, yolov5m.pt, yolov5l.pt, yolov5x.pt, yolov5n.pt\n",
        "  # Project : Name of the folder where you will keep the results of your different models\n",
        "  # Name : Name of that specific model you are running\n",
        "!python train.py --hyp hyp.scratch-low.yaml --epochs 100 --data road_damage.yaml --weights yolov5l.pt --cache --project 'RDD - Yolov5' --name 'third_training_5l_100e_scrlow'"
      ],
      "metadata": {
        "id": "8HdzaPrlUQgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c21b7a59-aca5-473a-ca50-3baa5bba7009"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/RDD_Final/yolov5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5l.pt, cfg=, data=road_damage.yaml, hyp=hyp.scratch-low.yaml, epochs=100, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=RDD - Yolov5, name=third_training_5l_100e_scrlow, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "Command 'git fetch origin' timed out after 5 seconds\n",
            "YOLOv5 ðŸš€ v6.2-94-g1aea74c Python-3.7.13 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs in Weights & Biases\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 ðŸš€ in ClearML\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir RDD - Yolov5', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 48.3MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5l.pt to yolov5l.pt...\n",
            "100% 89.3M/89.3M [00:17<00:00, 5.50MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=11\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
            "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
            "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
            "  7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
            "  8                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
            "  9                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]               \n",
            " 10                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
            " 14                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
            " 18                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
            " 21                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  3   9971712  models.common.C3                        [1024, 1024, 3, False]        \n",
            " 24      [17, 20, 23]  1     86160  models.yolo.Detect                      [11, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
            "Model summary: 468 layers, 46192144 parameters, 46192144 gradients, 108.4 GFLOPs\n",
            "\n",
            "Transferred 607/613 items from yolov5l.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 101 weight(decay=0.0), 104 weight(decay=0.0005), 104 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/RoadDamageDataset/Train' images and labels...16832 found, 0 missing, 5178 empty, 0 corrupt: 100% 16832/16832 [00:09<00:00, 1801.23it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/RoadDamageDataset/Train/Japan_006536.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/RoadDamageDataset/Train/Japan_006916.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: /content/RoadDamageDataset/Train/Japan_011427.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/RoadDamageDataset/Train.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (8.4GB ram):  41% 6824/16832 [01:04<04:21, 38.20it/s]^C\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}